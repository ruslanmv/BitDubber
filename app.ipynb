{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added DISPLAY configuration to /home/ruslanmv/.bashrc.\n",
      "Please run the following command to apply the changes:\n",
      "source /home/ruslanmv/.bashrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from setup import configure_display\n",
    "configure_display()\n",
    "import os\n",
    "# Use bash explicitly to source the .bashrc file\n",
    "os.system(\"bash -c 'source ~/.bashrc'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyautogui pillow pandas gradio requests python-dotenv ibm-watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Blog/Building-LLM-from-Scratch-in-Python/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import base64\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "#import pyautogui\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from ibm_watson import SpeechToTextV1, TextToSpeechV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "# IBM Watson TTS and STT Setup\n",
    "tts_authenticator = IAMAuthenticator(os.getenv('TTS_API_KEY'))\n",
    "stt_authenticator = IAMAuthenticator(os.getenv('STT_API_KEY'))\n",
    "text_to_speech = TextToSpeechV1(authenticator=tts_authenticator)\n",
    "text_to_speech.set_service_url(os.getenv('TTS_URL'))\n",
    "speech_to_text = SpeechToTextV1(authenticator=stt_authenticator)\n",
    "speech_to_text.set_service_url(os.getenv('STT_URL'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert text to speech\n",
    "def convert_text_to_speech(text, output_file=\"output.wav\"):\n",
    "    with open(output_file, 'wb') as audio_file:\n",
    "        response = text_to_speech.synthesize(\n",
    "            text=text,\n",
    "            voice='en-US_AllisonVoice',\n",
    "            accept='audio/wav'\n",
    "        ).get_result()\n",
    "        audio_file.write(response.content)\n",
    "    logging.info(f\"Converted text to speech and saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 23:25:26,182 - INFO - Converted text to speech and saved to output.wav\n"
     ]
    }
   ],
   "source": [
    "convert_text_to_speech(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert speech to text\n",
    "def convert_speech_to_text(audio_file_path):\n",
    "    with open(audio_file_path, 'rb') as audio_file:\n",
    "        response = speech_to_text.recognize(\n",
    "            audio=audio_file,\n",
    "            content_type='audio/wav',\n",
    "            timestamps=True,\n",
    "            word_confidence=True\n",
    "        ).get_result()\n",
    "    text = response['results'][0]['alternatives'][0]['transcript']\n",
    "    logging.info(f\"Converted speech to text: {text}\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 23:26:23,979 - INFO - Converted speech to text: hello \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_speech_to_text(\"output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to take a screenshot\n",
    "def take_screenshot(output_path=\"screenshot.png\"):\n",
    "    try:\n",
    "        pyautogui.screenshot(output_path)\n",
    "        logging.info(f\"Screenshot saved to {output_path}\")\n",
    "        return output_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error taking screenshot: {e}\")\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Function to encode the image in base64\n",
    "def get_encoded_string(file_name):\n",
    "    try:\n",
    "        with open(file_name, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error encoding image: {e}\")\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Function to get the IBM Cloud access token\n",
    "def get_auth_token(api_key):\n",
    "    url = \"https://iam.cloud.ibm.com/identity/token\"\n",
    "    data = {\"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\", \"apikey\": api_key}\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    response = requests.post(url, data=data, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"access_token\"]\n",
    "\n",
    "# Function to send the screenshot to LLaMA 3.2 for UI element detection\n",
    "def identify_ui_elements_with_llama32(screenshot_path):\n",
    "    try:\n",
    "        api_key = os.getenv(\"WATSONX_APIKEY\")\n",
    "        url_base = os.getenv(\"WATSONX_URL\")\n",
    "        access_token = get_auth_token(api_key)\n",
    "        encoded_image = get_encoded_string(screenshot_path)\n",
    "\n",
    "        #url = \"https://eu-de.ml.cloud.ibm.com/ml/v1/text/chat?version=2023-05-29\"\n",
    "        url = f\"{url_base}/ml/v1/text/chat?version=2023-05-29\"\n",
    "        body = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"Extract UI elements from the image.\"},\n",
    "                {\"role\": \"user\", \"content\": encoded_image}\n",
    "            ],\n",
    "            \"project_id\": os.getenv(\"PROJECT_ID\"),\n",
    "            \"model_id\": \"meta-llama/llama-3-2-90b-vision-instruct\",\n",
    "        }\n",
    "        headers = {\"Authorization\": f\"Bearer {access_token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "        response = requests.post(url, json=body, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Simulate output\n",
    "        elements = [\n",
    "            {\"coordinates\": (50, 100), \"description\": \"URL Bar\"},\n",
    "            {\"coordinates\": (200, 300), \"description\": \"Search Button\"}\n",
    "        ]\n",
    "        df = pd.DataFrame(elements)\n",
    "        csv_path = \"ui_elements.csv\"\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        logging.info(f\"UI elements saved to {csv_path}\")\n",
    "        return csv_path\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error identifying UI elements: {e}\")\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Function to determine click sequence using LLaMA 3.1\n",
    "def determine_click_sequence_with_llama31(user_request, csv_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        api_key = os.getenv(\"WATSONX_APIKEY\")\n",
    "        url_base = os.getenv(\"WATSONX_URL\")\n",
    "        access_token = get_auth_token(api_key)\n",
    "\n",
    "        # Build the prompt\n",
    "        prompt = f\"Given the UI elements: {df.to_dict(orient='records')}, which actions can be performed to execute the request: '{user_request}'? Respond in JSON format.\"\n",
    "        url = f\"{url_base}/ml/v1/text/chat?version=2023-05-29\"\n",
    "        #url = \"https://eu-de.ml.cloud.ibm.com/ml/v1/text/chat?version=2023-05-29\"\n",
    "        body = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"Provide a JSON response detailing actions based on UI elements.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"project_id\": os.getenv(\"PROJECT_ID\"),\n",
    "            \"model_id\": \"meta-llama/llama-3-1-90b-instruct\",\n",
    "        }\n",
    "        headers = {\"Authorization\": f\"Bearer {access_token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "        response = requests.post(url, json=body, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Simulate JSON response\n",
    "        actions = [\n",
    "            {\"action\": \"click\", \"coordinates\": (50, 100)},\n",
    "            {\"action\": \"type\", \"text\": \"https://www.wikipedia.org\"},\n",
    "            {\"action\": \"press\", \"key\": \"enter\"}\n",
    "        ]\n",
    "        logging.info(\"Click sequence determined.\")\n",
    "        return actions\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error determining click sequence: {e}\")\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Function to execute the click sequence\n",
    "def execute_click_sequence(click_sequence):\n",
    "    try:\n",
    "        for action in click_sequence:\n",
    "            if action[\"action\"] == \"click\":\n",
    "                pyautogui.moveTo(*action[\"coordinates\"])\n",
    "                pyautogui.click()\n",
    "            elif action[\"action\"] == \"type\":\n",
    "                pyautogui.typewrite(action[\"text\"])\n",
    "            elif action[\"action\"] == \"press\":\n",
    "                pyautogui.press(action[\"key\"])\n",
    "        logging.info(\"Execution completed.\")\n",
    "        return \"Execution completed.\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error executing click sequence: {e}\")\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Gradio Interface\n",
    "def handle_user_request(audio_input):\n",
    "    # Convert speech to text\n",
    "    user_request = convert_speech_to_text(audio_input)\n",
    "\n",
    "    # Take a screenshot\n",
    "    screenshot_path = take_screenshot()\n",
    "    if \"Error\" in screenshot_path:\n",
    "        return screenshot_path\n",
    "\n",
    "    # Identify UI elements\n",
    "    csv_path = identify_ui_elements_with_llama32(screenshot_path)\n",
    "    if \"Error\" in csv_path:\n",
    "        return csv_path\n",
    "\n",
    "    # Determine click sequence\n",
    "    click_sequence = determine_click_sequence_with_llama31(user_request, csv_path)\n",
    "    if not click_sequence or isinstance(click_sequence, str):\n",
    "        return f\"Error determining actions: {click_sequence}\"\n",
    "\n",
    "    # Convert planned actions to speech\n",
    "    actions_text = \"Planned sequence of actions: \" + \", \".join([str(action) for action in click_sequence])\n",
    "    convert_text_to_speech(actions_text)\n",
    "\n",
    "    # Confirm execution\n",
    "    confirm = input(\"Do you want to execute these actions? (yes/no): \")\n",
    "    if confirm.lower() != \"yes\":\n",
    "        return \"Execution cancelled by the user.\"\n",
    "\n",
    "    # Execute actions\n",
    "    return execute_click_sequence(click_sequence)\n",
    "\n",
    "# Gradio Interface Setup\n",
    "interface = gr.Interface(\n",
    "    fn=handle_user_request,\n",
    "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"BitDubber: Automated UI Interaction with Speech\",\n",
    "    description=\"Speak a command, and the program will identify UI elements and perform actions accordingly.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
